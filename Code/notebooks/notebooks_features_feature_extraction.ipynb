{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo\n",
    "- Look into acceleration data for selecting unusable areas\n",
    "- Look into RPeaks extraction and PAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyedflib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e82671e5336c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyedflib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfftpack\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfft\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyedflib'"
     ]
    }
   ],
   "source": [
    "import pytz\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "import pyedflib\n",
    "import scipy.signal as sig\n",
    "from scipy.fftpack import fft\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import biosppy\n",
    "\n",
    "import import_ipynb\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.preprocessing_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Experiment Base_Path|\n",
    "with open('../../config.json') as f:\n",
    "    paths = json.load(f)\n",
    "exp_base_path = paths['experiments']\n",
    "print(exp_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_date = '2019-02-01/Data'\n",
    "exp_patient = '002'\n",
    "\n",
    "patient_base_path = os.path.join(exp_base_path, exp_date, exp_patient)\n",
    "\n",
    "faros_files = {\n",
    "    'EDF': '.EDF',\n",
    "    'ASC': '.ASC',\n",
    "    'SDF': '.SDF'\n",
    "}\n",
    "if os.path.exists(os.path.join(patient_base_path, 'Faros_001')):\n",
    "    for file in os.listdir(os.path.join(patient_base_path, 'Faros_001')):\n",
    "        if file.endswith(\".EDF\"):\n",
    "            faros_files['EDF'] = file\n",
    "        elif file.endswith(\".ASC\"):\n",
    "            faros_files['ASC'] = file\n",
    "        elif file.endswith(\".SDF\"):\n",
    "            faros_files['SDF'] = file\n",
    "\n",
    "filelist = {\n",
    "    'faros_edf': os.path.join(patient_base_path, 'Faros_001', faros_files['EDF']),\n",
    "    'faros_asc': os.path.join(patient_base_path, 'Faros_001', faros_files['ASC']),\n",
    "    'faros_sdf': os.path.join(patient_base_path, 'Faros_001', faros_files['SDF']),\n",
    "    'empatica_acc': glob.glob(os.path.join(patient_base_path, 'Empatica_001/*ACC.csv'))[0],\n",
    "    'empatica_bvp': glob.glob(os.path.join(patient_base_path, 'Empatica_001/*BVP.csv'))[0],\n",
    "    #'empatica_eda': glob.glob(os.path.join(patient_base_path, '001/*EDA.csv')),\n",
    "    #'empatica_hr': glob.glob(os.path.join(patient_base_path, '001/*HR.csv')),\n",
    "    #'empatica_ibi': glob.glob(os.path.join(patient_base_path, '001/*IBI.csv')),\n",
    "    #'empatica_temp': glob.glob(os.path.join(patient_base_path, '001/*TEMP.csv')),\n",
    "    #'everion_signals': os.path.join(patient_base_path, 'Everion/CsvData_signals_EV-EC39-7079-57A8.csv'),\n",
    "    #'everion_sensors': os.path.join(patient_base_path, 'Everion/CsvData_sensor_data_EV-EC39-7079-57A8.csv'),\n",
    "    #'everion_aggregates': os.path.join(patient_base_path, 'Everion/CsvData_aggregates_EV-EC39-7079-57A8.csv'),\n",
    "    #'everion_analytics': os.path.join(patient_base_path, 'Everion/CsvData_analytics_events_EV-EC39-7079-57A8.csv'),\n",
    "    #'everion_events': os.path.join(patient_base_path, 'Everion/CsvData_everion_events_EV-EC39-7079-57A8.csv'),\n",
    "    #'everion_features': os.path.join(patient_base_path, 'Everion/CsvData_features_EV-EC39-7079-57A8.csv'),\n",
    "    'blood_pressure': glob.glob(os.path.join(patient_base_path, 'OMRON_001/*Processed.csv'))[0]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Blood Pressure Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bp_data(window_size, verbose=False):\n",
    "    df = pd.read_csv(filelist['blood_pressure'], sep = \",\", parse_dates=[0])\n",
    "    df.rename(columns=lambda x: x.strip().replace(\"\\t\",\"\"), inplace = True)\n",
    "    #df['Sys(mmHg)'] = df['Sys(mmHg)'].str.replace(\"\\t\",\"\")\n",
    "    df['Body Movement'] = df['BodyMovement'].str.replace(\"\\t\",\"\")\n",
    "    df['Step'] = df['Step'].str.replace(\"\\t\",\"\")\n",
    "    bp_df = df.where(df[\"Step\"] == 'BP Measurement').dropna()\n",
    "    #pd.to_datetime(bp_df[\"Date and Time of Measurement\"][-4:])\n",
    "    #pd.to_datetime(bp_df[\"Date and Time of Measurement\"].str[-8:])\n",
    "    bp_df['Measurement Date'] = bp_df[\"DateandTimeofMeasurement\"].str[-8:]\n",
    "    bp_df['Measurement Date'] = '02/01/2019 ' + bp_df['Measurement Date'].astype(str)\n",
    "    bp_df['Measurement Date'] = pd.to_datetime(bp_df['Measurement Date'])\n",
    "    bp_df['Measurement Date'] = bp_df['Measurement Date'] - pd.Timedelta('1 hour')\n",
    "    bp_df['Window End'] = bp_df['Measurement Date'] + window_size // 2\n",
    "    bp_df['Window Start'] = bp_df['Measurement Date'] - window_size // 2\n",
    "    if verbose:\n",
    "        bp_df.head(50)\n",
    "        \n",
    "    return bp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read PPG Data from Empatica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_empatica_df(bp_df, verbose=False):\n",
    "    signal_names = ['acc', 'bvp']\n",
    "    signals = {}\n",
    "    start_times = {}\n",
    "    sample_freqs = {}\n",
    "\n",
    "    # Check start dates\n",
    "    for signal in signal_names:\n",
    "        filename = filelist['empatica_{}'.format(signal)]\n",
    "        with open(filename, 'r') as f:\n",
    "            ts_line = f.readline()\n",
    "            start_times[signal] = int(ts_line[:ts_line.find('.')])\n",
    "            sf_line = f.readline()\n",
    "            sample_freqs[signal] = int(sf_line[:sf_line.find('.')])\n",
    "            split = re.split('[,\\n]', f.read())\n",
    "            split = np.array(split[:-1]).astype(float)\n",
    "            if signal == 'acc':\n",
    "                split = np.reshape(split, [-1, 3])\n",
    "            signals[signal] = split\n",
    "\n",
    "    signals['acc_mag'] = np.linalg.norm(signals['acc'], axis=1)\n",
    "    del signals['acc']\n",
    "\n",
    "    sample_freqs['acc_mag'] = sample_freqs['acc']\n",
    "    del sample_freqs['acc']\n",
    "\n",
    "    start_times['acc_mag'] = start_times['acc']\n",
    "    del start_times['acc']\n",
    "\n",
    "    signal_names.remove('acc')\n",
    "    signal_names.append('acc_mag')\n",
    "\n",
    "    data_empatica = utils.create_df(signal_names, signals, sample_freqs, start_times)\n",
    "\n",
    "    sub_data_empatica = pd.DataFrame()\n",
    "    for _, row in bp_df.iterrows():\n",
    "        truncated_df = data_empatica.truncate(before=row['Window Start'], after=row['Window End'])\n",
    "        if verbose:\n",
    "            utils.plot_signal(truncated_df.interpolate(method='index'), 'bvp', 'acc_mag')\n",
    "        sub_data_empatica = sub_data_empatica.append(truncated_df)\n",
    "    if verbose:\n",
    "        print(sub_data_empatica.head())\n",
    "        print(sub_data_empatica.describe())\n",
    "        utils.plot_signal(sub_data_empatica.interpolate(method='index'), 'bvp', 'acc_mag')\n",
    "    return sub_data_empatica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_faros_df(bp_df, verbose=False):\n",
    "    with pyedflib.EdfReader(filelist['faros_edf']) as f:\n",
    "        # read EDF file into a dictionary, keys: signal_labels \n",
    "        n = f.signals_in_file    # get num of signal types \n",
    "        signal_labels = f.getSignalLabels()   # get labels for types of signals\n",
    "        n_samples = f.getNSamples()\n",
    "        raw_sample_freqs = f.getSampleFrequencies()\n",
    "        \n",
    "        ### Dealing with timezone issues\n",
    "        tz = pytz.timezone('Europe/Berlin')\n",
    "        initial_ts = datetime.datetime.fromtimestamp(0, tz)\n",
    "        signal_ts = f.getStartdatetime().replace(tzinfo=pytz.UTC)\n",
    "        ###\n",
    "        \n",
    "        start_ts = (signal_ts - initial_ts).total_seconds()\n",
    "        print(start_ts)\n",
    "        raw_data = dict.fromkeys(signal_labels)   # the signals has difference sizes, therefore put into a dictionary\n",
    "        for i in np.arange(n):\n",
    "            raw_data[signal_labels[i]] = f.readSignal(i)\n",
    "\n",
    "    raw_data['acc_mag'] = np.linalg.norm([raw_data['Accelerometer_X'], raw_data['Accelerometer_Y'], raw_data['Accelerometer_Z']], axis=0)\n",
    "    signal_names = ['ECG', 'acc_mag', 'HRV']\n",
    "    sample_freqs = {}\n",
    "    for i in range(len(signal_labels)):\n",
    "        sample_freqs[signal_labels[i]] = raw_sample_freqs[i]\n",
    "    sample_freqs['acc_mag'] = sample_freqs['Accelerometer_X']\n",
    "    start_timestamps = {\n",
    "        'ECG': start_ts,\n",
    "        'acc_mag': start_ts,\n",
    "        'HRV': start_ts\n",
    "    }\n",
    "    data_faros = utils.create_df(signal_names, raw_data, sample_freqs, start_timestamps)\n",
    "    \n",
    "    sub_data_faros = pd.DataFrame()\n",
    "    for _, row in bp_df.iterrows():\n",
    "        truncated_df = data_faros.truncate(before=row['Window Start'], after=row['Window End'])\n",
    "        if verbose:\n",
    "            utils.plot_signal(truncated_df.interpolate(method='index'), 'ECG', 'acc_mag')\n",
    "        sub_data_faros = sub_data_faros.append(truncated_df)\n",
    "    if verbose:\n",
    "        print(data_faros.head())\n",
    "        print(data_faros.describe())\n",
    "        \n",
    "    return sub_data_faros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots dft of a signal\n",
    "def plot_dft(signal, freq):\n",
    "    N = len(signal)\n",
    "    T = 1/freq\n",
    "    yf = fft(signal)\n",
    "    xf = np.linspace(0.0, 1.0/(2.0*T), N//2)\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(xf, 2.0/N * np.abs(yf[0:N//2]))\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Empatica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds the local minima that correspond to the starts of a cardiac cycle\n",
    "def find_cycle_starts(df, sample_rate=64):\n",
    "    minima = sig.find_peaks(-df.values, distance=0.7*sample_rate)[0] # Todo: Check other parameters\n",
    "    #diffs = np.diff(minima)\n",
    "    #minima = np.delete(minima, np.argwhere(diffs < 0.75*np.mean(diffs)))\n",
    "    return minima\n",
    "\n",
    "# returns the x values for those samples in the signal, that are closest to some given y value\n",
    "def find_xs_for_y(ys, y_val, sys_peak):\n",
    "    diffs = abs(ys - y_val)\n",
    "    x1 = diffs[:sys_peak].idxmin()\n",
    "    x2 = diffs[sys_peak:].idxmin()\n",
    "    return x1, x2\n",
    "\n",
    "# takes a dataframe of calculated features and removes the outliers occuding due to inaccuracies in the signal\n",
    "def clean_window_features_of_outliers(df):\n",
    "    quant = df.quantile(0.8)\n",
    "    for col in df.columns:\n",
    "        if col.find('ts') == -1:\n",
    "            df = df[df[col] < quant[col]*2]\n",
    "    return df\n",
    "\n",
    "# finds sections with high acceleration magnitude and removes them\n",
    "def remove_motion_sections(df, limit=100, min_size=5, padding=15, std_mult=0.25):\n",
    "    # Todo check frequency domain for high frequencies\n",
    "    acc_mag_mean = df['acc_mag'].mean()\n",
    "    acc_mag_std =  df['acc_mag'].std()\n",
    "    # Comparison with overall mean and std\n",
    "    thresh_indices = np.squeeze(np.argwhere((df['acc_mag'] > acc_mag_mean + std_mult * acc_mag_std) | \n",
    "                                            (df['acc_mag'] < acc_mag_mean - std_mult * acc_mag_std)))\n",
    "        \n",
    "    section_indices = []\n",
    "    section_start = thresh_indices[0]\n",
    "    for i in range(1, len(thresh_indices) - 1):\n",
    "        if thresh_indices[i] - thresh_indices[i-1] > limit:\n",
    "            if thresh_indices[i-1] >= section_start + min_size:\n",
    "                section_indices.append((section_start - padding, thresh_indices[i-1] + padding))\n",
    "            section_start = thresh_indices[i]\n",
    "    if thresh_indices[-1] != section_start:\n",
    "        section_indices.append((section_start, thresh_indices[-1]))\n",
    "    \n",
    "    # Check local variance for window\n",
    "    #section_indices = []\n",
    "    #window_vars = []\n",
    "    #print(df.count()['bvp'])\n",
    "    #for i in range((df.count()['bvp'] - limit) // padding + 1):\n",
    "    #    window = df['acc_mag'].iloc[i*padding:i*padding+limit]\n",
    "    #    var = window.var()\n",
    "    #    window_vars.append(var)\n",
    "    #ind = np.squeeze(np.argwhere(window_vars > np.float64(1)))\n",
    "    #print(ind)\n",
    "    #section_indices = []\n",
    "    #section_start = ind[0]\n",
    "    #for i in range(1, len(ind) - 1):\n",
    "    #    if ind[i] - ind[i-1] > 1:\n",
    "    #        section_indices.append((section_start*padding, ind[i-1]*padding+limit))\n",
    "    #        section_start = ind[i]\n",
    "    #if ind[-1] != section_start:\n",
    "    #    section_indices.append((section_start*padding, ind[-1]*padding+limit))\n",
    "    #print(section_indices)\n",
    "        \n",
    "    section_indices.reverse()\n",
    "    for (start, end) in section_indices:\n",
    "        df = df.drop(index=df.iloc[start:end].index)\n",
    "    return df\n",
    "\n",
    "def find_clean_cycles_with_template(signal, verbose=False):\n",
    "    initial_cycle_starts = find_cycle_starts(signal)\n",
    "    if len(initial_cycle_starts) <= 1:\n",
    "        return []\n",
    "    template_length = math.floor(np.median(np.diff(initial_cycle_starts)))\n",
    "    cycle_starts = initial_cycle_starts[:-1]\n",
    "    while cycle_starts[-1] + template_length > len(signal):\n",
    "        cycle_starts = cycle_starts[:-1]\n",
    "    template = []\n",
    "    for i in range(template_length):\n",
    "        template.append(np.mean(signal[cycle_starts + i]))\n",
    "    \n",
    "    corr_coef = []\n",
    "    for cycle_start in cycle_starts:\n",
    "        corr_coef.append(np.corrcoef(template, signal[cycle_start:cycle_start+template_length])[0,1])\n",
    "\n",
    "    valid_indices = np.argwhere(np.array(corr_coef) >= 0.8)\n",
    "    if (len(valid_indices) > len(cycle_starts) / 2) and len(valid_indices) > 1:\n",
    "        cycle_starts = cycle_starts[np.squeeze(valid_indices)]\n",
    "        template2 = []\n",
    "        for i in range(template_length):\n",
    "            template2.append(np.mean(signal[cycle_starts + i]))\n",
    "        template = template2\n",
    "        \n",
    "    if verbose:\n",
    "        print('Cycle Template')\n",
    "        plt.plot(template)\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    # Check correlation of cycles with template\n",
    "    # SQI1: Pearson Correlation\n",
    "    sqi1_corr = []\n",
    "    for cycle_start in cycle_starts:\n",
    "        corr, _ = stats.pearsonr(template, signal[cycle_start:cycle_start+template_length])\n",
    "        sqi1_corr.append(corr)\n",
    "        \n",
    "    # SQI2: Pearson Correlation with \n",
    "    sqi2_corr = []\n",
    "    for cycle_start in cycle_starts:\n",
    "        cycle_end = initial_cycle_starts[np.squeeze(np.argwhere(initial_cycle_starts==cycle_start)) + 1] \n",
    "        corr, _ = stats.pearsonr(template, sig.resample(signal[cycle_start:cycle_end], template_length))\n",
    "        sqi2_corr.append(corr)\n",
    "        \n",
    "    # SQI3: Pearson Correlation\n",
    "    #sq2_corr = []\n",
    "    #for cycle_start in cycle_starts:\n",
    "    #    cycle_end = initial_cycle_starts[initial_cycle_starts.index(cycle_start) + 1] \n",
    "    #    corr, _ = stats.pearsonr(template, df['bvp'].iloc[cycle_start:cycle_end])\n",
    "    #    sq2_corr.append(corr)\n",
    "    corrs = np.array([sqi1_corr, sqi2_corr]).transpose()\n",
    "    cycle_starts = cycle_starts[np.all(corrs >= 0.8, axis=1)]\n",
    "    \n",
    "    if verbose:\n",
    "        print('Detected Valid Cycles')\n",
    "        plt.figure()\n",
    "        for cycle_start in cycle_starts:\n",
    "            plt.plot(signal[cycle_start:cycle_start+template_length].to_numpy())\n",
    "        plt.show()\n",
    "        \n",
    "    cycles = []\n",
    "    for cycle_start in cycle_starts:\n",
    "        cycle_end = initial_cycle_starts[np.squeeze(np.argwhere(initial_cycle_starts==cycle_start)) + 1]\n",
    "        if (cycle_end - cycle_start) > template_length*1.2:\n",
    "            cycle_end = cycle_start + template_length\n",
    "        cycles.append((cycle_start, cycle_end))\n",
    "\n",
    "    return cycles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Faros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_r_peaks(ecg_signal):\n",
    "    rpeaks = biosppy.signals.ecg.christov_segmenter(ecg_signal, 1000)\n",
    "    return rpeaks[0]\n",
    "\n",
    "def add_rpeaks(data_faros, rpeaks): \n",
    "    data_faros['rpeaks'] = 0\n",
    "    data_faros.iloc[rpeaks, data_faros.columns.get_loc('rpeaks')] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter PPG/ECG Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_for_cycle(window_df, signal, verbose=False):\n",
    "    cur_index = window_df.index.max() + 1\n",
    "    if np.isnan(cur_index):\n",
    "        cur_index = 0\n",
    "    signal = signal.resample('ms').nearest(limit=1).interpolate(method='time')\n",
    "    signal = signal - signal.min()\n",
    "    max_amplitude = signal.max()\n",
    "    \n",
    "    peaks = sig.find_peaks(signal.values)[0]\n",
    "    sys_peak_ts = signal.index[peaks[0]]\n",
    "    \n",
    "    if verbose:\n",
    "        plt.figure()\n",
    "        plt.xlim((signal.index.min(), signal.index.max()))\n",
    "        plt.scatter(signal.index[peaks], signal[peaks])\n",
    "        plt.plot(signal.index, signal.values)\n",
    "    # Features\n",
    "    window_df = window_df.append(pd.DataFrame({'start_ts': signal.index.min(),\n",
    "                                               'sys_peak_ts': sys_peak_ts,\n",
    "                                               'T_S': (sys_peak_ts - signal.index.min()).total_seconds(),\n",
    "                                               'T_D': (signal.index.max() - sys_peak_ts).total_seconds()\n",
    "                                              }, index=[cur_index]), sort=False)\n",
    "    for p in [10, 25, 33, 50, 66, 75]:\n",
    "        p_ampl = p / 100 * max_amplitude\n",
    "        x1, x2 = find_xs_for_y(signal, p_ampl, peaks[0])\n",
    "        if verbose:\n",
    "            plt.scatter([x1, x2], signal[[x1, x2]])\n",
    "        window_df.loc[cur_index, 'DW_'+str(p)] = (x2 - sys_peak_ts).total_seconds()\n",
    "        window_df.loc[cur_index, 'DW_SW_sum_'+str(p)] = (x2 - x1).total_seconds()\n",
    "        window_df.loc[cur_index, 'DW_SW_ratio_'+str(p)] = (x2 - sys_peak_ts) / (sys_peak_ts - x1)\n",
    "    if verbose:\n",
    "        plt.show()\n",
    "    return window_df\n",
    "    \n",
    "def extract_features_for_window(df, verbose=False):\n",
    "    cycles = find_clean_cycles_with_template(df['bvp_normalised'], verbose=verbose)\n",
    "    if len(cycles) == 0:\n",
    "        return pd.DataFrame()\n",
    "    #plt.figure(figsize=(16, 6))\n",
    "    #plt.plot(df['bvp'].to_numpy())\n",
    "    #plt.scatter(cycle_starts, df['bvp'].iloc[cycle_starts])\n",
    "    #plt.show()\n",
    "    \n",
    "    window_features = pd.DataFrame()\n",
    "    cur_index = 0\n",
    "    for i in range(len(cycles)):\n",
    "        window_features = extract_features_for_cycle(window_features, df['bvp_normalised'].iloc[cycles[i][0]:cycles[i][1]], verbose=verbose)\n",
    "        if i > 0:\n",
    "            window_features.loc[cur_index-1, 'CP'] = (window_features.loc[cur_index, 'sys_peak_ts'] - window_features.loc[cur_index-1, 'sys_peak_ts']).total_seconds()\n",
    "        cur_index = cur_index + 1\n",
    "    if verbose:\n",
    "        print('Cycle Features within Window:')\n",
    "        print(window_features)\n",
    "    window_features = clean_window_features_of_outliers(window_features)\n",
    "    return window_features\n",
    "\n",
    "def apply_filters(df):\n",
    "    # No smoothing neccessary due to relatively low sampling frequency\n",
    "    df['bvp_normalised'] = (df['bvp'] - df['bvp'].min()) / (df['bvp'].max() - df['bvp'].min())\n",
    "    #butter_f_b, butter_f_a = sig.butter(N=3, Wn=[0.5, 5], btype='bandpass', fs=64)\n",
    "    #df['bvp_filtered'] = sig.filtfilt(butter_f_b, butter_f_a, df['bvp_normalised'])\n",
    "    #plt.figure(figsize=(14, 6))\n",
    "    #plt.plot(df.index, df['bvp_normalised'])\n",
    "    return df\n",
    "\n",
    "def extract_features_for_signal(signal, bp, verbose=False):\n",
    "    for index, row in bp.iterrows():\n",
    "        window_df = signal.truncate(before=row['Window Start'], after=row['Window End'])\n",
    "        window_features = extract_features_for_window(window_df, verbose)\n",
    "        for col in window_features.columns:\n",
    "            if col.find('ts') == -1:\n",
    "                bp.loc[index, col+'_mean'] = window_features[col].mean()\n",
    "                bp.loc[index, col+'_var'] = window_features[col].var()\n",
    "    bp.dropna(inplace=True)\n",
    "    return bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_PAT_for_window(ppg_df, ecg_df, verbose=False):\n",
    "    #plt.plot(ppg_df['bvp_normalised'])\n",
    "    cycles = find_clean_cycles_with_template(ppg_df['bvp_normalised'], verbose=verbose)\n",
    "    ppg_peak_tss = []\n",
    "    for i in range(len(cycles)):\n",
    "        signal = ppg_df['bvp_normalised'].iloc[cycles[i][0]:cycles[i][1]].resample('ms').nearest(limit=1).interpolate(method='time')\n",
    "        signal = signal - signal.min()\n",
    "        peaks = sig.find_peaks(signal.values)[0]\n",
    "        ppg_peak_tss.append(signal.index[peaks[0]])\n",
    "    PAT_times = []\n",
    "    # Check if there is an R-Peak before the first detected PPG peak \n",
    "    if len(ppg_peak_tss) > 0 and (ppg_peak_tss[0] < ecg_df.index[ecg_df['rpeaks'] == 1]).all():\n",
    "        del ppg_peak_tss[0]\n",
    "    if len(ppg_peak_tss) > 0:\n",
    "        ecg_peak_tss = []\n",
    "        for ppg_peak_ts in ppg_peak_tss:\n",
    "            ecg_peak_indices = ecg_df.index[(ecg_df['rpeaks'] == 1) & (ecg_df.index < ppg_peak_ts)]\n",
    "            if len(ecg_peak_indices) > 0:\n",
    "                ecg_peak_tss.append(ecg_peak_indices[-1])\n",
    "        PAT_timedeltas = np.array(ppg_peak_tss) - np.array(ecg_peak_tss)\n",
    "        for td in PAT_timedeltas:\n",
    "            PAT_times.append(td.total_seconds())\n",
    "        if verbose:\n",
    "            fig, ax1 = plt.subplots(figsize=(15, 6))\n",
    "            ax2 = ax1.twinx()\n",
    "            ax2.plot(ppg_df['bvp_normalised'])\n",
    "            ax1.plot(ecg_df['ECG'], color='r')\n",
    "            ax2.scatter(ppg_peak_tss, [0.8]*len(ppg_peak_tss))\n",
    "            ax2.scatter(ecg_peak_tss, [0.8]*len(ecg_peak_tss), color='r')\n",
    "            fig.tight_layout()\n",
    "            plt.show()\n",
    "            print(PAT_times)\n",
    "    return PAT_times\n",
    "    \n",
    "def extract_PAT(ppg_signal, ecg_signal, bp, verbose=False):\n",
    "    for index, row in bp.iterrows():\n",
    "        window_ppg = ppg_signal.truncate(before=row['Window Start'], after=row['Window End'])\n",
    "        window_ecg = ecg_signal.truncate(before=row['Window Start'], after=row['Window End'])\n",
    "        window_PATs = extract_PAT_for_window(window_ppg, window_ecg, verbose)\n",
    "        bp.loc[index, 'PAT_mean'] = np.mean(window_PATs)\n",
    "        bp.loc[index, 'PAT_var'] = np.var(window_PATs)\n",
    "    bp.dropna(inplace=True)\n",
    "    return bp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_df = load_bp_data(pd.Timedelta('1 min'))\n",
    "sub_data_empatica = load_empatica_df(bp_df)\n",
    "#utils.plot_signal(sub_data_empatica, 'bvp')\n",
    "sub_data_empatica = remove_motion_sections(sub_data_empatica)\n",
    "sub_data_empatica = apply_filters(sub_data_empatica)\n",
    "#utils.plot_signal(sub_data_empatica, 'bvp')\n",
    "#utils.plot_signal(sub_data_empatica, 'bvp_normalised')\n",
    "\n",
    "features = extract_features_for_signal(sub_data_empatica, bp_df, verbose=True)\n",
    "features.to_csv('../../data/processed/features_{}.csv'.format(exp_patient), index=False)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_df = load_bp_data(pd.Timedelta('10 sec'))\n",
    "sub_data_faros = load_faros_df(bp_df)\n",
    "rpeaks = extract_r_peaks(sub_data_faros['ECG'].values)\n",
    "add_rpeaks(sub_data_faros, rpeaks.tolist())\n",
    "sub_data_empatica = load_empatica_df(bp_df)\n",
    "#utils.plot_signal(sub_data_empatica, 'bvp')\n",
    "sub_data_empatica = remove_motion_sections(sub_data_empatica)\n",
    "sub_data_empatica = apply_filters(sub_data_empatica)\n",
    "\n",
    "features = extract_PAT(sub_data_empatica, sub_data_faros, bp_df, verbose=True)\n",
    "features.to_csv('../../data/processed/pat_features_{}.csv'.format(exp_patient), index=False)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wfdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "# record_name number_of_signals sampling_frequency number_of_samples_per_signal\n",
    "# file_name format initial_value checksum block_size description\n",
    "records = wfdb.io.rdrecord('../../../../Downloads/ecg_data')\n",
    "print(records.p_signal.shape)\n",
    "records.p_signal = records.p_signal[100000:101000]\n",
    "wfdb.plot.plot_wfdb(records, figsize=(16, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records2 = wfdb.io.rdrecord('../../../../Downloads/s0010_re')\n",
    "print(records2.p_signal.shape)\n",
    "print(vars(records2))\n",
    "wfdb.plot.plot_wfdb(records2, figsize=(16, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LOAD_TYPES = {'8': '<i1', '16': '<i2', '24': '<i3', '32': '<i4',\n",
    "                   '61': '>i2', '80': '<u1', '160': '<u2', '212': '<u1',\n",
    "                   '310': '<u1', '311': '<u1'}\n",
    "fmt = '16'\n",
    "n_samp = 2500\n",
    "with open('../../../../Downloads/CU025130.821', 'rb') as fp:\n",
    "    fp.seek(0, 2)\n",
    "    print(fp.tell())\n",
    "    fp.seek(1000000)\n",
    "    sig_data = np.fromfile(fp, dtype=np.dtype(DATA_LOAD_TYPES[fmt]), count=n_samp)\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(sig_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
